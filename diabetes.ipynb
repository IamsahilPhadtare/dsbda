!pip install pyspark
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, when, sum, isnan, mode
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.types import IntegerType

# Initialize Spark session
spark = SparkSession.builder \
        .appName("Diabetes Readmission Prediction") \
        .getOrCreate()

# Loading Dataset
data = spark.read.csv("diabetic_data.csv", header=True, inferSchema=True)

# Drop irrelevant Columns
data = data.drop("encounter_id", "patient_nbr", "weight", "payer_code", "medical_specialty", "race")

# Handle missing values (replace with mean or mode for numerical/categorical features)
# For numerical columns
numerical_cols = ["num_lab_procedures", "num_procedures", "num_medications"]
for col_name in numerical_cols:
    mean_value = data.selectExpr(f"avg({col_name})").collect()[0][0]
    data = data.fillna(mean_value, subset=[col_name])

# For categorical columns
categorical_cols = ["diag_1", "diag_2", "diag_3"]
for col_name in categorical_cols:
    mode_value = data.groupBy(col_name).agg(mode(col_name).alias("mode")).collect()[0][0]
    data = data.fillna(mode_value, subset=[col_name])

# Replace specific values
data = data.withColumn("readmitted", when(col("readmitted") == ">30", "Yes").when(col("readmitted") == "<30", "Yes").otherwise(col("readmitted")))

# Encode categorical variables
categorical_cols = ["age", "gender", "diag_1", "diag_2", "diag_3", "readmitted", "diabetesMed"]
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+"_index", handleInvalid="keep") for col_name in categorical_cols]
encoders = [OneHotEncoder(inputCol=col_name+"_index", outputCol=col_name+"_encoded") for col_name in categorical_cols]

# Assemble feature vector
feature_cols = ["gender_encoded", "diag_1_encoded", "diag_2_encoded", "diag_3_encoded",
                "num_lab_procedures", "num_procedures", "num_medications"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features", handleInvalid="keep")

pipeline = Pipeline(stages=indexers + encoders + [assembler])
preprocessed_data = pipeline.fit(data).transform(data)

# Split data into train and test sets
train_data, test_data = preprocessed_data.randomSplit([0.7, 0.3], seed=42)

# Initialize classifiers
decision_tree = DecisionTreeClassifier(featuresCol="features", labelCol="readmitted_index", maxDepth=5)

# Train classifiers
dt_model = decision_tree.fit(train_data)

# Transform test data
test_data_transformed = dt_model.transform(test_data)

# Initialize the evaluator for multi-class classification
evaluator = MulticlassClassificationEvaluator(labelCol="readmitted_index", predictionCol="prediction", metricName="accuracy")

# Evaluate the model
accuracy = evaluator.evaluate(test_data_transformed)
print(f"Accuracy: {accuracy}")

from pyspark.ml.classification import DecisionTreeClassifier

# Assuming your trained model variable is named dt_model
# Replace "model_path.pkl" with the path where you want to save the pickle file
model_path = "/content/model_path.pkl"

# Save the model as a pickle file
dt_model.save(model_path)


import gradio as gr

# Define a function to make predictions
def predict_diabetes_readmission(age, gender, diag_1, diag_2, diag_3, num_lab_procedures, num_procedures, num_medications):
    # Create a Spark DataFrame from the input
    input_data = spark.createDataFrame([(age, gender, diag_1, diag_2, diag_3, num_lab_procedures, num_procedures, num_medications)], 
                                        ["age", "gender", "diag_1", "diag_2", "diag_3", "num_lab_procedures", "num_procedures", "num_medications"])
    # Preprocess the input data
    preprocessed_input = pipeline.transform(input_data)
    
    # Make predictions using the trained decision tree model
    predictions = dt_model.transform(preprocessed_input)
    
    # Extract the predicted class
    prediction = predictions.select("prediction").collect()[0][0]
    
    # Map the predicted class index to the original class label
    predicted_class = "Yes" if prediction == 1.0 else "No"
    
    return predicted_class

# Create Gradio interface
iface = gr.Interface(fn=predict_diabetes_readmission, 
                     inputs=["text", "text", "text", "text", "text", "number", "number", "number"], 
                     outputs="text", 
                     title="Diabetes Readmission Prediction",
                     description="Enter patient information to predict readmission status.")

# Launch the interface
iface.launch()
